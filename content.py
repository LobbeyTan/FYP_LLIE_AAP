title = "Toward Simultaneous Grayscale Low-Light Enhancement and Color Restoration"

abstract = "The study of low-light image enhancements (LLIE) is to translate images from low light distribution to normal light distribution which could recover the important details in the image and improve the visibility of the objects in the image. The enhanced image is used to improve performance of high-level computer vision algorithms under low light condition such as object detection, face recognition, image segmentation and others. Thus far, there are different kinds of algorithms being proposed to tackle this problem and extensive researches have been done to improve the enhancement results which can be generally categorized as histogram equalization, dehazing, Retinex method, and deep learning method. The deep learning method has proved to be able to achieve better enhancement results and many state-of-the-art algorithms are based on deep neural networks. In this work, we propose to use unsupervised Generative Adversarial Network (GAN) that can be trained without paired low-to-normal light datasets for simultaneous low-light enhancement and colour restoration. We transform all low-light images to grayscale and train the model to translate input images to normal light distribution with colour restoration. To best of our knowledge, this is the first work that successfully learn low-light grayscale enhancement and colour restoration to produce normal light RGB images. By just changing input images to grayscale, we can reduce the complexity of the model without affecting much on the enhancement performance. Extensive experiments show that our method have acceptable performance on low light image enhancement, but it outperforms the state-of-the-art approaches in low light object detection."

keywords = "Keywords: Low-light image enhancements, low light object detection, grayscale style transfer, Generative Adversarial Network (GAN), unsupervised learning, image-to-image translation."

network_architecture = "Our GAN model is built on top of EnlightenGAN (Jiang, et al., 2019)[4] which consist of one generator and two discriminators to direct global and local information. The generator is implemented with Unet with Residual Block architecture and is guided by our proposed custom attention module. As shown in figure below, the proposed generator architecture diagram, the generator consists of convolutional blocks, attentional blocks, upsampling layers and downsampling layers."

enhanced_result_desc = "We compared our model results on low-light enhancement with six different state-of-the-art methods, including Illumination Map Estimation based approach (LIME)[3], Weighted Variational Model for Simultaneous Reflectance and Illumination Estimation (SRIE)[1], Deep Retinex Decomposition method (RetinexNet)[6], Kindling the Darkness with Retinex theory (KinD)[7], unsupervised GAN training without Paired Supervision (EnlightenGAN)[4] and Zero-Reference Deep Curve Estimation (ZeroDCE)[2]."

detection_result_desc = "After getting the enhanced low-light images from all the models, we then use the pretrained YoloV7 for object detection on those enhanced results. The object detection is running on Exclusive Dark (ExDark)[5] dataset which the ground truth labels are present. We use the mean average precison (mAP) and mean average recall (mAR) to validate the models performance."

powerpoint_embed_link = "https://56bfgb-my.sharepoint.com/personal/lobbeytan_56bfgb_onmicrosoft_com/_layouts/15/Doc.aspx?sourcedoc={df2a4a60-5fbe-4333-8f57-4dbdd930ab7c}&amp;action=embedview&amp;wdAr=1.7777777777777777&amp;wdEaaCheck=1"